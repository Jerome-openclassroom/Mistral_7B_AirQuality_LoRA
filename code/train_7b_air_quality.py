# -*- coding: utf-8 -*-
"""Mistral_7B_AirQuality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16bRrk_AF5F_r_ssr4xMSaVyuUWAzuiqJ
"""

!pip install -q --upgrade "transformers>=4.46.0" bitsandbytes peft datasets trl accelerate huggingface_hub

import transformers
import trl
import torch

print("transformers version :", transformers.__version__)
print("trl version          :", trl.__version__)
print("torch version        :", torch.__version__)
print("CUDA dispo:", torch.cuda.is_available())

import os, json

train_path = "/content/sample_data/lyra_air_sante_train_400.jsonl"
valid_path = "/content/sample_data/lyra_air_sante_valid_120.jsonl"

def check_jsonl(path, n=3):
    print(f"\nüîç V√©rification du fichier : {path}")
    if not os.path.exists(path):
        print("‚ùå Fichier introuvable.")
        return False
    try:
        with open(path, "r", encoding="utf-8") as f:
            lines = [next(f).strip() for _ in range(n)]
        print(f"‚úÖ Fichier trouv√© ({os.path.getsize(path)/1024:.1f} Ko)")
        for i, line in enumerate(lines, 1):
            data = json.loads(line)
            print(f"‚Äî Ligne {i} OK, cl√©s principales :", list(data.keys()))
        print("‚úÖ Structure JSONL valide (au moins sur les 3 premi√®res lignes).")
        return True
    except Exception as e:
        print("‚ö†Ô∏è Erreur :", e)
        return False

# Test des deux chemins
ok_train = check_jsonl(train_path)
ok_valid = check_jsonl(valid_path)

if ok_train and ok_valid:
    print("\nüöÄ Les deux datasets sont valides et pr√™ts √† √™tre charg√©s.")
else:
    print("\n‚ö†Ô∏è Probl√®me d√©tect√© sur au moins un des fichiers.")

import torch, os
from google.colab import userdata
from huggingface_hub import login

base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"

print("GPU :", torch.cuda.get_device_name(0))
print("CUDA dispo:", torch.cuda.is_available())

hf_token = userdata.get('TOKEN_HF_AirSante')
if not hf_token:
    raise ValueError("‚ùå Le secret TOKEN_HF_AirSante est introuvable ou vide.")
os.environ["HF_TOKEN"] = hf_token
login(token=os.environ["HF_TOKEN"])
print("üîê HF OK")

import json
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer

train_path = "/content/sample_data/lyra_air_sante_train_400.jsonl"
valid_path = "/content/sample_data/lyra_air_sante_valid_120.jsonl"

def load_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

train_rows = load_jsonl(train_path)
valid_rows = load_jsonl(valid_path)

dataset = DatasetDict({
    "train": Dataset.from_list(train_rows),
    "validation": Dataset.from_list(valid_rows)
})

print("taille train :", len(dataset["train"]))
print("taille valid :", len(dataset["validation"]))
print("exemple brut :", dataset["train"][0])

tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

def to_chat_text(example):
    # On prend tel quel le tableau "messages" fourni dans le dataset
    # add_generation_prompt=False => inclut aussi la r√©ponse assistant
    txt = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=False,
        add_generation_prompt=False,
    )
    return {"text": txt}

dataset = dataset.map(to_chat_text)

print("aper√ßu text :", dataset["train"][0]["text"][:500])

from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    torch_dtype=torch.float16,
    device_map="auto",
)

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./lyra_air_sante_qLoRA",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    weight_decay=0.0,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,                          # Mistral 7B Instruct en 4 bits + LoRA
    processing_class=tokenizer,           # remplace l'ancien 'tokenizer='
    train_dataset=dataset["train"],       # dataset avec 'messages' + champ 'text'
    eval_dataset=dataset["validation"],
    args=training_args)


trainer.train()

output_dir = "./lyra_air_sante_qLoRA_adapter"

trainer.model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("Adapters sauvegard√©s dans", output_dir)
!ls -R {output_dir}

with open(f"{output_dir}/README.md", "w", encoding="utf-8") as f:
    f.write("""<colle ici le contenu du bloc README ci-dessus>""")

from huggingface_hub import HfApi, upload_folder, whoami

api = HfApi()
repo_name = "lyra_air_sante_mistral7B_qLoRA"
full_repo_id = f"jeromex1/{repo_name}"

print("üë§ Compte connect√© :", whoami()["name"])

# Cr√©e le repo (remplace 'name' par 'repo_id')
api.create_repo(
    repo_id=full_repo_id,
    token=os.environ["HF_TOKEN"],
    private=False,
    exist_ok=True
)

# Upload du dossier contenant les adapters et le README
upload_folder(
    repo_id=full_repo_id,
    folder_path="./lyra_air_sante_qLoRA_adapter",
    commit_message="Initial qLoRA adapters for lyra_air_sante (Mistral-7B-Instruct-v0.3) + model card",
    token=os.environ["HF_TOKEN"]
)

print(f"üöÄ Adapters publi√©s sur : https://huggingface.co/{full_repo_id}")