# -*- coding: utf-8 -*-
"""Test_inference_7B_AirSante.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y9fbumvoSf-_lU3NLXAMu1-IuCHRcdoX
"""

# üîê Connexion Hugging Face depuis les secrets Colab
from google.colab import userdata
from huggingface_hub import login
import os


import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# 0. Auth HF
hf_token = userdata.get('TOKEN_HF_AirSante')
if not hf_token:
    raise ValueError("‚ùå Le secret 'TOKEN_HF_AirSante' est introuvable ou vide.")
os.environ["HF_TOKEN"] = hf_token
login(token=os.environ["HF_TOKEN"])
print("üîê Authentification Hugging Face OK")

base_model_id = "mistralai/Mistral-7B-Instruct-v0.3"
adapter_id    = "jeromex1/lyra_air_sante_mistral7B_qLoRA"

# 1. tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# 2. r√©pertoire d'offload pour les couches CPU
offload_dir = "/content/offload_dir"
os.makedirs(offload_dir, exist_ok=True)

# 3. charger le mod√®le EN FP16, sans quantization, avec device_map="auto"
#    -> transformers va r√©partir les couches entre GPU et CPU et utiliser offload_dir
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    offload_folder=offload_dir,
)

# 4. appliquer l'adapter LoRA
model = PeftModel.from_pretrained(
    base_model,
    adapter_id,
    device_map="auto",
    offload_folder=offload_dir,
)
model.eval()

def generate_from_messages(messages, max_new_tokens=128, temperature=0.7, top_p=0.9):
    # messages = [{"role":"user","content":"..."}]
    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True  # dit au mod√®le "tu es l'assistant maintenant"
    )

    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            eos_token_id=tokenizer.eos_token_id,
        )

    text = tokenizer.decode(out[0], skip_special_tokens=True)
    return text

tests = [
    [
        {
            "role": "user",
            "content": (
                "Conditions m√©t√©o : Temp√©rature = 34¬∞C, Humidit√© = 45%, "
                "Inversion thermique = oui, Pollens = tr√®s √©lev√©, IQA = 6, "
                "Vent fort = non, Pluie forte = non.\n"
                "Quels sont les impacts potentiels sur la sant√© ?"
            )
        }
    ],
    [
        {
            "role": "user",
            "content": (
                "Conditions m√©t√©o : Temp√©rature = 10¬∞C, Humidit√© = 80%, "
                "Inversion thermique = non, Pollens = faible, IQA = 2, "
                "Vent fort = oui, Pluie forte = oui.\n"
                "Effets possibles sur les personnes √¢g√©es ?"
            )
        }
    ],
    [
        {
            "role": "user",
            "content": (
                "Conditions m√©t√©o : Temp√©rature = 28¬∞C, Humidit√© = 30%, "
                "Inversion thermique = oui, Pollens = moyen, IQA = 8, "
                "Vent fort = non, Pluie forte = non.\n"
                "Quels conseils pour les personnes asthmatiques ?"
            )
        }
    ],
    [
        {
            "role": "user",
            "content": (
                "Conditions m√©t√©o : Temp√©rature = 22¬∞C, Humidit√© = 60%, "
                "Inversion thermique = non, Pollens = √©lev√©, IQA = 4, "
                "Vent fort = oui, Pluie forte = non.\n"
                "Risque pour les travailleurs en ext√©rieur ?"
            )
        }
    ],
    [
        {
            "role": "user",
            "content": (
                "Conditions m√©t√©o : Temp√©rature = 36¬∞C, Humidit√© = 50%, "
                "Inversion thermique = oui, Pollens = faible, IQA = 7, "
                "Vent fort = non, Pluie forte = non.\n"
                "Quels effets combin√©s chaleur + pollution pr√©voir ?"
            )
        }
    ],
]

for i, m in enumerate(tests, 1):
    print(f"\nüß© Test {i}")
    print("Question utilisateur :")
    print(m[0]["content"])
    print("\nüß† R√©ponse du mod√®le :")
    print(generate_from_messages(m))
    print("-" * 80)
