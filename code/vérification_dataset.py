# -*- coding: utf-8 -*-
"""Vérification_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xwrbapUaltQLXacLUysY2-sNyLUgMVk9
"""

pip install sentence-transformers

import os
print(os.getcwd())
os.listdir()

import re

def parse_conditions(user_text):
    """
    Extrait les paramètres structurés depuis le texte du user.
    Retourne un dict de features normalisées.
    """
    def find(pattern, default=None, cast=None):
        m = re.search(pattern, user_text)
        if not m:
            return default
        val = m.group(1).strip()
        if cast:
            try:
                return cast(val)
            except:
                return default
        return val

    # Température = "10-20°C" -> on peut prendre la moyenne 15, par exemple
    temp_range = find(r"Température\s*=\s*([0-9]+)-([0-9]+)°C")
    if temp_range:
        m = re.search(r"Température\s*=\s*([0-9]+)-([0-9]+)°C", user_text)
        t1 = int(m.group(1))
        t2 = int(m.group(2))
        temp_mean = (t1 + t2) / 2.0
    else:
        temp_mean = None

    # Humidité = "<40%" ou ">70%" etc. -> on extrait le nombre et le signe
    humidity_raw = find(r"Humidité\s*=\s*([^,]+)")
    # On va normaliser : "<40%" -> 40 avec un flag "moins_que"
    hum_val = None
    hum_sign = None
    if humidity_raw:
        m = re.search(r"([<>]?)(\d+)", humidity_raw)
        if m:
            hum_sign = m.group(1) if m.group(1) != "" else "="
            hum_val = int(m.group(2))

    inv_therm = 1 if re.search(r"Inversion thermique\s*=\s*oui", user_text) else 0
    pollens = find(r"Pollens\s*=\s*([^,]+)", default=None)
    iqa = find(r"IQA\s*=\s*([0-9]+)", default=None, cast=int)
    vent_fort = 1 if re.search(r"Vent fort\s*=\s*oui", user_text) else 0
    pluie_forte = 1 if re.search(r"Pluie forte\s*=\s*oui", user_text) else 0

    return {
        "temp_mean": temp_mean,
        "hum_val": hum_val,
        "hum_sign": hum_sign,          # "<", ">", "="
        "inv_therm": inv_therm,
        "pollens": pollens,            # ex: "faible", "élevé"
        "iqa": iqa,
        "vent_fort": vent_fort,
        "pluie_forte": pluie_forte
    }

import json
import pandas as pd

def load_air_with_features(file_path):
    rows = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            obj = json.loads(line.strip())
            user_msg = obj["messages"][0]["content"]

            feats = parse_conditions(user_msg)
            feats["line_index"] = i
            feats["user_raw"] = user_msg  # on garde le texte original pour inspection

            rows.append(feats)

    df_feat = pd.DataFrame(rows)
    return df_feat

import math

def pair_similarity(row_a, row_b, temp_tol=1.0, hum_tol=5, iqa_tol=1):
    score = 0
    total = 0

    # Température moyenne (tolérance en °C)
    if row_a["temp_mean"] is not None and row_b["temp_mean"] is not None:
        total += 1
        if abs(row_a["temp_mean"] - row_b["temp_mean"]) <= temp_tol:
            score += 1

    # Humidité valeur (tolérance en %)
    if row_a["hum_val"] is not None and row_b["hum_val"] is not None:
        total += 1
        if abs(row_a["hum_val"] - row_b["hum_val"]) <= hum_tol:
            score += 1

    # Signe d'humidité (<, >, =)
    if row_a["hum_sign"] is not None and row_b["hum_sign"] is not None:
        total += 1
        if row_a["hum_sign"] == row_b["hum_sign"]:
            score += 1

    # Inversion thermique (0/1)
    total += 1
    if row_a["inv_therm"] == row_b["inv_therm"]:
        score += 1

    # Pollens (faible / élevé / etc.)
    if row_a["pollens"] is not None and row_b["pollens"] is not None:
        total += 1
        if row_a["pollens"] == row_b["pollens"]:
            score += 1

    # IQA (tolérance de 1 point)
    if row_a["iqa"] is not None and row_b["iqa"] is not None:
        total += 1
        if abs(row_a["iqa"] - row_b["iqa"]) <= iqa_tol:
            score += 1

    # Vent fort (0/1)
    total += 1
    if row_a["vent_fort"] == row_b["vent_fort"]:
        score += 1

    # Pluie forte (0/1)
    total += 1
    if row_a["pluie_forte"] == row_b["pluie_forte"]:
        score += 1

    # Similarité finale = score / total (entre 0 et 1)
    if total == 0:
        return 0.0
    return score / total

def find_near_duplicates(df_feat, threshold=0.99):
    near_pairs = []
    for i in range(len(df_feat)):
        for j in range(i+1, len(df_feat)):
            sim = pair_similarity(df_feat.iloc[i], df_feat.iloc[j])
            if sim >= threshold:
                near_pairs.append({
                    "line_index_1": int(df_feat.iloc[i]["line_index"]),
                    "line_index_2": int(df_feat.iloc[j]["line_index"]),
                    "similarity_custom": sim,
                    "user_1": df_feat.iloc[i]["user_raw"],
                    "user_2": df_feat.iloc[j]["user_raw"]
                })
    return near_pairs

df_feat = load_air_with_features("sample_data/lyra_air_sante_train_400.jsonl")

near_pairs = find_near_duplicates(df_feat, threshold=0.99)

print(f"Paires quasi identiques détectées : {len(near_pairs)}\n")
for p in near_pairs[:20]:  # éviter d'exploser l'output
    print(f"Lignes {p['line_index_1']} / {p['line_index_2']} (sim={p['similarity_custom']:.2f})")
    print("  1:", p['user_1'])
    print("  2:", p['user_2'])
    print("")

# --- CELLULE À AJOUTER APRÈS TON CODE EXISTANT ---

!pip install -q sentence-transformers

import torch
from sentence_transformers import SentenceTransformer, util

########################################
# 1. Recharger (ou étendre) le dataset avec la réponse assistant
########################################

def load_air_with_features_and_outputs(file_path):
    rows = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            obj = json.loads(line.strip())

            user_msg = obj["messages"][0]["content"]
            assistant_msg = obj["messages"][1]["content"] if len(obj["messages"]) > 1 else ""

            feats = parse_conditions(user_msg)
            feats["line_index"] = i
            feats["user_raw"] = user_msg
            feats["assistant_raw"] = assistant_msg

            rows.append(feats)

    df_feat_full = pd.DataFrame(rows)
    return df_feat_full

# On recharge avec les sorties
df_full = load_air_with_features_and_outputs("sample_data/lyra_air_sante_train_400.jsonl")

########################################
# 2. Construire les embeddings des entrées (user) et sorties (assistant)
########################################

# Modèle d'embedding phrase-level. (petit, rapide)
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# Embeddings utilisateur (entrées) et assistant (sorties)
user_embeddings = embed_model.encode(
    df_full["user_raw"].tolist(),
    convert_to_tensor=True,
    normalize_embeddings=True
)

assistant_embeddings = embed_model.encode(
    df_full["assistant_raw"].tolist(),
    convert_to_tensor=True,
    normalize_embeddings=True
)

########################################
# 3. Fonctions utilitaires de similarité cosinus
########################################

def cosine_sim(a, b):
    # a, b: tenseurs 1D
    # retourne un scalaire float Python
    return float(util.cos_sim(a, b).cpu().numpy()[0][0])


########################################
# 4. Analyse croisée cohérence entrée/sortie
########################################

def analyze_input_output_consistency(
    df,
    user_embeds,
    assistant_embeds,
    input_close_thresh=0.98,
    input_far_thresh=0.70,
    output_close_thresh=0.95,
    output_far_thresh=0.80,
    max_pairs_report=50
):
    """
    Cherche les deux cas problématiques :

    Cas A : Entrées TROP proches -> Sorties TROP différentes
        - similarité_input >= input_close_thresh
        - similarité_output <= output_far_thresh

    Cas B : Entrées TROP différentes -> Sorties TROP proches
        - similarité_input <= input_far_thresh
        - similarité_output >= output_close_thresh

    Les seuils sont ajustables.
    """

    alerts_A = []  # input quasi identiques -> output divergentes
    alerts_B = []  # input différentes -> output quasi identiques

    n = len(df)

    for i in range(n):
        for j in range(i+1, n):

            sim_in = cosine_sim(user_embeds[i], user_embeds[j])
            sim_out = cosine_sim(assistant_embeds[i], assistant_embeds[j])

            # Cas A
            if sim_in >= input_close_thresh and sim_out <= output_far_thresh:
                alerts_A.append({
                    "line_i": int(df.iloc[i]["line_index"]),
                    "line_j": int(df.iloc[j]["line_index"]),
                    "sim_input": sim_in,
                    "sim_output": sim_out,
                    "user_i": df.iloc[i]["user_raw"],
                    "user_j": df.iloc[j]["user_raw"],
                    "assistant_i": df.iloc[i]["assistant_raw"],
                    "assistant_j": df.iloc[j]["assistant_raw"]
                })

            # Cas B
            if sim_in <= input_far_thresh and sim_out >= output_close_thresh:
                alerts_B.append({
                    "line_i": int(df.iloc[i]["line_index"]),
                    "line_j": int(df.iloc[j]["line_index"]),
                    "sim_input": sim_in,
                    "sim_output": sim_out,
                    "user_i": df.iloc[i]["user_raw"],
                    "user_j": df.iloc[j]["user_raw"],
                    "assistant_i": df.iloc[i]["assistant_raw"],
                    "assistant_j": df.iloc[j]["assistant_raw"]
                })

    # tri pour rendre lisible : on veut d'abord les cas les plus suspects
    alerts_A = sorted(alerts_A, key=lambda x: (-(x["sim_input"]), x["sim_output"]))
    alerts_B = sorted(alerts_B, key=lambda x: (x["sim_input"], -(x["sim_output"])))

    # print résumé
    print("=== CAS A : entrées quasi identiques -> sorties divergentes ===")
    print(f"Total détecté : {len(alerts_A)}\n")
    for a in alerts_A[:max_pairs_report]:
        print(f"Lignes {a['line_i']} / {a['line_j']}")
        print(f"sim_input={a['sim_input']:.3f}  sim_output={a['sim_output']:.3f}")
        print("USER i :", a["user_i"])
        print("USER j :", a["user_j"])
        print("ASSIST i:", a["assistant_i"])
        print("ASSIST j:", a["assistant_j"])
        print("")

    print("\n=== CAS B : entrées très différentes -> sorties quasi identiques ===")
    print(f"Total détecté : {len(alerts_B)}\n")
    for b in alerts_B[:max_pairs_report]:
        print(f"Lignes {b['line_i']} / {b['line_j']}")
        print(f"sim_input={b['sim_input']:.3f}  sim_output={b['sim_output']:.3f}")
        print("USER i :", b["user_i"])
        print("USER j :", b["user_j"])
        print("ASSIST i:", b["assistant_i"])
        print("ASSIST j:", b["assistant_j"])
        print("")

    # on renvoie aussi les listes brutes si tu veux les manipuler ensuite
    return alerts_A, alerts_B


########################################
# 5. Exécution de l'analyse
########################################

alerts_A, alerts_B = analyze_input_output_consistency(
    df_full,
    user_embeddings,
    assistant_embeddings,
    input_close_thresh=0.98,   # très proches en entrée
    input_far_thresh=0.70,     # assez différentes en entrée
    output_close_thresh=0.95,  # très proches en sortie
    output_far_thresh=0.80,    # très différentes en sortie
    max_pairs_report=20        # limite d'affichage pour colab
)

# --- CELLULE : analyse de la dominance statistique des profils environnementaux ---

import pandas as pd

# 1. Normalisation douce des pollens (simplifie les valeurs textuelles)
def normalize_pollen(val):
    if val is None or pd.isna(val):
        return "inconnu"
    v = val.lower().strip()
    if "élev" in v:
        return "élevé"
    if "modér" in v or "moy" in v:
        return "modéré"
    if "faib" in v or "bas" in v:
        return "faible"
    return v

# 2. Créer une colonne "profil" condensée
def make_profile(row):
    # IQA : on le regroupe par tranches de 25 (0–25, 26–50, 51–75, etc.)
    iqa_band = None
    if pd.notna(row["iqa"]):
        iqa_band = f"{(row['iqa']//25)*25}-{((row['iqa']//25)+1)*25 - 1}"
    else:
        iqa_band = "inconnu"

    return (
        f"IQA={iqa_band}, "
        f"Pollens={normalize_pollen(row['pollens'])}, "
        f"InvTherm={'oui' if row['inv_therm']==1 else 'non'}, "
        f"VentFort={'oui' if row['vent_fort']==1 else 'non'}, "
        f"PluieForte={'oui' if row['pluie_forte']==1 else 'non'}"
    )

df_full["profil_env"] = df_full.apply(make_profile, axis=1)

# 3. Comptage des occurrences de chaque profil
profil_counts = df_full["profil_env"].value_counts().reset_index()
profil_counts.columns = ["profil_env", "nombre_occurrences"]

# 4. Pourcentage du dataset total
total = len(df_full)
profil_counts["pourcentage"] = (profil_counts["nombre_occurrences"] / total * 100).round(2)

# 5. Affichage synthétique
print(f"=== Répartition des profils environnementaux (total {total} lignes) ===\n")
display(profil_counts.head(20))  # affiche les 20 plus fréquents dans Colab

# 6. (Optionnel) Indication si dominance d’un profil particulier
max_share = profil_counts["pourcentage"].max()
dominant_profile = profil_counts.iloc[0]["profil_env"]
if max_share > 20:
    print(f"⚠️  Le profil dominant ({dominant_profile}) représente {max_share:.1f}% du dataset.")
else:
    print(f"✅  Aucun profil ne dépasse 20% du dataset ({max_share:.1f}% max).")

from matplotlib import pyplot as plt
_df_0['nombre_occurrences'].plot(kind='hist', bins=20, title='nombre_occurrences')
plt.gca().spines[['top', 'right',]].set_visible(False)

# --- CELLULE : visualisation des 10 profils environnementaux les plus fréquents ---

import matplotlib.pyplot as plt

# On garde les 10 profils les plus représentés
top_n = 10
df_top = profil_counts.head(top_n).iloc[::-1]  # inversion pour affichage du + petit au + grand (esthétique horizontale)

# Taille de la figure
plt.figure(figsize=(8, 5))

# Bar chart horizontal
bars = plt.barh(df_top["profil_env"], df_top["pourcentage"], color="#4C72B0", alpha=0.85)

# Ajout des annotations (pourcentages à droite des barres)
for bar, pct in zip(bars, df_top["pourcentage"]):
    plt.text(
        bar.get_width() + 0.3,             # décalage horizontal
        bar.get_y() + bar.get_height()/2,  # centrage vertical
        f"{pct:.1f}%",
        va='center',
        fontsize=9,
        color="#333333"
    )

# Titres et labels
plt.title("Top 10 des profils environnementaux les plus fréquents", fontsize=12, weight="bold")
plt.xlabel("Pourcentage du dataset (%)")
plt.ylabel("Profil environnemental")
plt.grid(axis="x", linestyle="--", alpha=0.4)

plt.tight_layout()
plt.show()